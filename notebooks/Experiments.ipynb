{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7051f6ec",
      "metadata": {
        "id": "7051f6ec"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "opcnI--8qTNQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opcnI--8qTNQ",
        "outputId": "0aa6143c-4fb5-4bc4-99ad-452f5bbd6191"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Tua95NAwQQn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Tua95NAwQQn",
        "outputId": "cbe95de5-d0c4-4c0d-e573-a5b6a40c4705"
      },
      "outputs": [],
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/emmarex/plantdisease\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe82621",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fe82621",
        "outputId": "d78dd372-0aee-45ed-c2d6-4507fb77573e"
      },
      "outputs": [],
      "source": [
        "class PlantDiseaseDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load the image and apply transformations\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "def load_images(directory_root):\n",
        "    image_list, label_list = [], []\n",
        "    print(\"[INFO] Loading images...\")\n",
        "\n",
        "    for disease_folder in os.listdir(directory_root):\n",
        "        disease_folder_path = os.path.join(directory_root, disease_folder)\n",
        "        if not os.path.isdir(disease_folder_path):\n",
        "            continue\n",
        "\n",
        "        for img_name in os.listdir(disease_folder_path):\n",
        "            if img_name.startswith(\".\"):\n",
        "                continue\n",
        "            img_path = os.path.join(disease_folder_path, img_name)\n",
        "            if img_path.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "                image_list.append(img_path)\n",
        "                label_list.append(disease_folder)\n",
        "\n",
        "    print(\"[INFO] Image loading completed\")\n",
        "    print(f\"Total images: {len(image_list)}\")\n",
        "    return image_list, label_list\n",
        "\n",
        "# Load images and labels\n",
        "directory_root = \"/content/plantdisease/PlantVillage\" # Placeholder for your dataset path\n",
        "image_paths, labels = load_images(directory_root)\n",
        "\n",
        "# Encode labels as integers\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Train, validation, and test splits\n",
        "train_paths, temp_paths, train_labels, temp_labels = train_test_split(\n",
        "    image_paths, labels_encoded, test_size=0.3, random_state=42, stratify=labels_encoded\n",
        ")\n",
        "valid_paths, test_paths, valid_labels, test_labels = train_test_split(\n",
        "    temp_paths, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_paths)}\")\n",
        "print(f\"Validation samples: {len(valid_paths)}\")\n",
        "print(f\"Test samples: {len(test_paths)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eb4e025",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "1eb4e025",
        "outputId": "d66272e3-8403-44de-8d57-cc2b543af706"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_class_distribution(labels, class_names):\n",
        "    class_counts = Counter(labels)\n",
        "    sorted_counts = sorted(class_counts.items(), key=lambda x: x[0])\n",
        "    counts = [count for _, count in sorted_counts]\n",
        "    class_labels = [class_names[idx] for idx, _ in sorted_counts]\n",
        "    percentages = [count/len(labels)*100 for count in counts]\n",
        "\n",
        "    plt.figure(figsize=(10,8))\n",
        "    ax = plt.subplot(111, projection='polar')\n",
        "    angles = np.linspace(0, 2*np.pi, len(counts), endpoint=False)\n",
        "    colors = plt.cm.YlGn(np.linspace(0.4, 0.9, len(counts)))\n",
        "    bars = ax.bar(angles, counts, width=2*np.pi/len(counts)*0.75, alpha=0.8, color=colors)\n",
        "\n",
        "    for angle, count, percentage in zip(angles, counts, percentages):\n",
        "        label_radius = count + (max(counts) * 0.08)\n",
        "        label = f'{count:,}\\n({percentage:.1f}%)'\n",
        "        ax.text(angle, label_radius, label, ha='center', va='center', fontsize=10,\n",
        "                rotation=np.degrees(angle) if -90 <= np.degrees(angle) <= 90 else np.degrees(angle) + 180)\n",
        "\n",
        "    ax.set_title('Plant Disease Distribution\\nTotal Samples: {:,}'.format(sum(counts)), y=1.05, fontsize=20, pad=20)\n",
        "    ax.set_xticks(angles)\n",
        "    ax.set_xticklabels(class_labels, fontsize=10)\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.set_ylim(0, max(counts) * 1.2)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plt.style.use('classic')\n",
        "plt.rcParams['figure.facecolor'] = 'white'\n",
        "visualize_class_distribution(train_labels, label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9b865b0",
      "metadata": {
        "id": "f9b865b0"
      },
      "outputs": [],
      "source": [
        "# Data Transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),           # Data augmentation for training\n",
        "    transforms.RandomRotation(30),              # Random rotation for variability\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
        "])\n",
        "\n",
        "valid_test_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),              # Consistent resizing for validation/test\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Same normalization as training\n",
        "])\n",
        "\n",
        "# Create datasets with appropriate transformations\n",
        "train_dataset = PlantDiseaseDataset(train_paths, train_labels, transform=train_transform)\n",
        "valid_dataset = PlantDiseaseDataset(valid_paths, valid_labels, transform=valid_test_transform)\n",
        "test_dataset = PlantDiseaseDataset(test_paths, test_labels, transform=valid_test_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle for training\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) # No shuffle for validation/test\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   # No shuffle for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b3bc44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "f7b3bc44",
        "outputId": "57bccbe1-9aff-4d3c-9038-b022351b479e"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(dataset, num_samples=5):\n",
        "    plt.figure(figsize=(22, 5))\n",
        "    for i in range(num_samples):\n",
        "        img, label = dataset[i]\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        img = (img * 0.5) + 0.5  # De-normalize\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(label_encoder.inverse_transform([label])[0].replace('_',' '))  # Convert label back to class name\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "visualize_samples(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee1e00a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cee1e00a",
        "outputId": "4bcb868e-64c1-430c-afad-3baeb2e6fd37"
      },
      "outputs": [],
      "source": [
        "for inputs, labels in test_loader:\n",
        "    print(f\"Batch inputs shape: {inputs.shape}\")  # Should be [batch_size, 3, 128, 128]\n",
        "    print(f\"Batch labels shape: {labels.shape}\")  # Should be [batch_size]\n",
        "    print(f\"First 5 samples labels: {labels[:5]}\")   # Print first 5 labels\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92280542",
      "metadata": {
        "id": "92280542"
      },
      "outputs": [],
      "source": [
        "class CustomCNN(nn.Module):\n",
        "    def __init__(self, num_clasess):\n",
        "        super(CustomCNN, self).__init__()\n",
        "        # Convolution Block 1\n",
        "        self.conv_block1=nn.Sequential(\n",
        "            nn.Conv2d(3,32, kernel_size=3, padding=\"same\"),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)  # Output: 32x128x128 (assuming input is 3x256x256)\n",
        "        )\n",
        "        # Convolution Block 2\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding='same'),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2) # Output: 64x64x64\n",
        "        )\n",
        "\n",
        "        # Convolution Block 3\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=\"same\"),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2) # Output: 128x32x32\n",
        "        )\n",
        "        # Convolution Block 4\n",
        "        self.conv_block4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=\"same\"),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2) # Output: 256x16x16\n",
        "        )\n",
        "        # Global Average Pooling\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1,1)) # Output: 256x1x1\n",
        "        # Fully Connected Layers\n",
        "        self.fc_block = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256, 128), # Adjusted input size after GAP\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(128,num_clasess)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.conv_block1(x)\n",
        "        x = self.conv_block2(x)\n",
        "        x = self.conv_block3(x)\n",
        "        x = self.conv_block4(x)\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = self.fc_block(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "195d1531",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "195d1531",
        "outputId": "a2c37e05-54c6-453b-fb38-e0e013cbf626"
      },
      "outputs": [],
      "source": [
        "# Initialize model, loss, optimizer\n",
        "num_clasess = len(label_encoder.classes_)\n",
        "print(f\"Number of Clasess: {num_clasess}\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device Used: {device}\")\n",
        "\n",
        "model = CustomCNN(num_clasess).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.002)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a88d76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48a88d76",
        "outputId": "4db17677-16f9-4bfa-e3d2-bd72d488ef62"
      },
      "outputs": [],
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3,256,256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05a4ad87",
      "metadata": {
        "id": "05a4ad87"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience =3, min_delta=0, save_path=\"best_model.pth\"):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.save_path = save_path\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "            torch.save(model.state_dict(), self.save_path)  # Save the best model\n",
        "            print(f\"[INFO] Model checkpoint saved to {self.save_path}\")\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                print(\"[INFO] Early stopping triggered.\")\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_loss /= len(data_loader)\n",
        "    accuracy = correct / total *100\n",
        "    return val_loss, accuracy\n",
        "\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, epochs, early_stopping=None):\n",
        "    train_losses,train_accuarcies, valid_losses, valid_accuracies = [], [], [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        progress_bar = tqdm(enumerate(train_loader), desc=f\"Epoch {epoch+1}/{epochs}\", total=len(train_loader))\n",
        "\n",
        "        for batch_idx, (inputs, labels) in progress_bar:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            running_corrects+= torch.sum(preds == labels.data)\n",
        "            progress_bar.set_postfix({'Train Loss': loss.item()})\n",
        "\n",
        "        # Record training loss\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "        train_accuracy = running_corrects.double() / len(train_loader.dataset) * 100\n",
        "        train_accuarcies.append(train_accuracy.item())\n",
        "\n",
        "        # Validation step\n",
        "        val_loss, val_accuracy = evaluate_model(model, valid_loader, criterion)\n",
        "        valid_losses.append(val_loss)\n",
        "        valid_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.2f}%\")\n",
        "\n",
        "        # Early stopping\n",
        "        if early_stopping and early_stopping(val_loss, model):\n",
        "            print(\"[INFO] Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "    return train_losses,train_accuarcies, valid_losses, valid_accuracies\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1_rOrKFR_WUn",
      "metadata": {
        "id": "1_rOrKFR_WUn"
      },
      "source": [
        "**TRAINING of The CNN model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c305466",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c305466",
        "outputId": "09800d3e-dfb8-4f74-8e50-c15f32e0c29e"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "early_stopping = EarlyStopping(patience=4, min_delta=0.01, save_path=\"best_model.pth\")\n",
        "train_losses, valid_losses, valid_accuracies = train_model(\n",
        "    model, train_loader, valid_loader, criterion, optimizer, epochs=n_epochs, early_stopping=early_stopping\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd08850",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "ddd08850",
        "outputId": "b32a1320-cd12-4f20-e4e1-1da63d8d8f7c"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curve(train_losses,train_accuarcies, valid_losses, valid_accuracies):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Loss curve\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label=\"Train Loss\")\n",
        "    plt.plot(valid_losses, label=\"Validation Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss Curve\")\n",
        "    plt.legend()\n",
        "\n",
        "    # Accuracy curve\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(valid_accuracies, label=\"Validation Accuracy\")\n",
        "    plt.plot(train_accuarcies, label=\"Train Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.title(\"Accuracy Curve\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curve(train_losses, valid_losses, valid_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4aef6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa4aef6f",
        "outputId": "4f6157c4-dd03-444e-80fb-0ad1eb5cc20e"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "print(\"[INFO] Best model loaded for final evaluation.\")\n",
        "\n",
        "final_val_loss, final_val_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "print(f\"Final Evaluation -> test Loss: {final_val_loss:.4f}, test Accuracy: {final_val_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54985415",
      "metadata": {},
      "source": [
        "# EFFICIENTNET MODEL\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MLlgyziK3vro",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLlgyziK3vro",
        "outputId": "9b7ea2e7-0830-4fb8-c954-65cc0c1d9571"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "from torchsummary import summary\n",
        "\n",
        "enet_model = models.efficientnet_b0(weights=True)\n",
        "enet_model = enet_model.to(device) # Move the model to the device (GPU)\n",
        "summary(enet_model, input_size=(3,256,256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nkaug2z8DsWM",
      "metadata": {
        "id": "Nkaug2z8DsWM"
      },
      "outputs": [],
      "source": [
        "# Freezing the layers\n",
        "for param in enet_model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "doJ0WX5cD2JE",
      "metadata": {
        "id": "doJ0WX5cD2JE"
      },
      "outputs": [],
      "source": [
        "num_ftrs = enet_model.classifier[1].in_features\n",
        "enet_model.classifier[1] = nn.Linear(num_ftrs, num_clasess)\n",
        "enet_model = enet_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IkO-2oJc3vdW",
      "metadata": {
        "id": "IkO-2oJc3vdW"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(enet_model.classifier[1].parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LtPkuuSl9NhA",
      "metadata": {
        "id": "LtPkuuSl9NhA"
      },
      "outputs": [],
      "source": [
        "efficientnet_train_losses, efficientnet_val_losses = [], []\n",
        "efficientnet_train_accuracies, efficientnet_val_accuracies = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HncBmd4fPEbs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HncBmd4fPEbs",
        "outputId": "efe8ead2-7060-485a-efd3-d71005856fff"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "\n",
        "early_stopping = EarlyStopping(patience=4, min_delta=0.01, save_path=\"best_enet_model.pth\")\n",
        "efficientnet_train_losses,efficientnet_train_accuracies, efficientnet_val_losses, efficientnet_val_accuracies = train_model(\n",
        "    enet_model, train_loader, valid_loader, criterion, optimizer, epochs=n_epochs, early_stopping=early_stopping\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SISLVMe8M2aw",
      "metadata": {
        "id": "SISLVMe8M2aw"
      },
      "outputs": [],
      "source": [
        "plot_learning_curve(efficientnet_train_losses,efficientnet_train_accuracies, efficientnet_val_losses, efficientnet_val_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AlqYEw6M2Sk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "7AlqYEw6M2Sk",
        "outputId": "7aeef708-c29f-4492-8669-e2b475b6ab17"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(\"best_enet_model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X4PcXnMlQahR",
      "metadata": {
        "id": "X4PcXnMlQahR"
      },
      "source": [
        "Testing on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RgLO4Cn6M2QC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgLO4Cn6M2QC",
        "outputId": "89a504bb-3a2d-4c4c-9c6d-1fbb2aee97ba"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "enet_model.load_state_dict(torch.load(\"best_enet_model.pth\"))\n",
        "print(\"[INFO] Best model loaded for final evaluation.\")\n",
        "\n",
        "final_test_loss, final_test_accuracy = evaluate_model(enet_model, test_loader, criterion)\n",
        "print(f\"Final Evaluation -> test Loss: {final_test_loss:.4f}, test Accuracy: {final_test_accuracy:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5431bb8",
      "metadata": {},
      "source": [
        "# DIET MODEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n-PPN790M2M0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-PPN790M2M0",
        "outputId": "526b1bf5-20a4-405a-97a7-07a0e640feef"
      },
      "outputs": [],
      "source": [
        "! pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mkDUMKubM2J1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkDUMKubM2J1",
        "outputId": "218a24ff-c8fe-41a5-ca3d-fa6ec714559a"
      },
      "outputs": [],
      "source": [
        "import timm\n",
        "\n",
        "num_classes = 15\n",
        "batch_size = 32\n",
        "epochs = 50\n",
        "lr = 1e-4\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device Used: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uw8_EPIgM1_Q",
      "metadata": {
        "id": "uw8_EPIgM1_Q"
      },
      "outputs": [],
      "source": [
        "# Data Transformations\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),           # Data augmentation for training\n",
        "    transforms.RandomRotation(30),              # Random rotation for variability\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard normalization\n",
        "])\n",
        "\n",
        "valid_test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),              # Consistent resizing for validation/test\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Same normalization as training\n",
        "])\n",
        "\n",
        "# Create datasets with appropriate transformations\n",
        "train_dataset = PlantDiseaseDataset(train_paths, train_labels, transform=train_transform)\n",
        "valid_dataset = PlantDiseaseDataset(valid_paths, valid_labels, transform=valid_test_transform)\n",
        "test_dataset = PlantDiseaseDataset(test_paths, test_labels, transform=valid_test_transform)\n",
        "\n",
        "# Create dataloaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle for training\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False) # No shuffle for validation/test\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)   # No shuffle for test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MPUKlxCuM18G",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPUKlxCuM18G",
        "outputId": "af105799-be17-4931-8551-80dbf84ad45b"
      },
      "outputs": [],
      "source": [
        "deit_model = timm.create_model(\n",
        "    \"deit_small_patch16_224\",\n",
        "    pretrained=True,\n",
        "    num_classes=num_classes\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vOUPwQ1u9NY4",
      "metadata": {
        "id": "vOUPwQ1u9NY4"
      },
      "outputs": [],
      "source": [
        "for param in deit_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "deit_model.head = nn.Linear(deit_model.head.in_features, num_classes)\n",
        "\n",
        "optimizer = torch.optim.AdamW(deit_model.head.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "deit_model = deit_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H3aTMDUg9NPn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "H3aTMDUg9NPn",
        "outputId": "8309a110-2e2b-4b09-ccda-d8db5fdaf291"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience=4, min_delta=0.01, save_path=\"best_deit_model.pth\")\n",
        "deit_train_losses,deit_train_accuracies, deit_val_losses, deit_val_accuracies = train_model(\n",
        "    deit_model, train_loader, valid_loader, criterion, optimizer, epochs=epochs, early_stopping=early_stopping\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a09a8273",
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_learning_curve(deit_train_losses,deit_train_accuracies, deit_val_losses, deit_val_accuracies)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
